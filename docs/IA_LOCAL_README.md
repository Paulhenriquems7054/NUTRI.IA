# ğŸ¤– IA Local Offline - Guia de IntegraÃ§Ã£o

Este documento descreve a integraÃ§Ã£o da IA Local Offline usando Ollama no Nutri.IA.

## ğŸ“‹ VisÃ£o Geral

A IA Local foi integrada de forma **nÃ£o invasiva** ao app existente, adicionando uma camada de fallback inteligente que:

1. **Prioriza IA Local** (Ollama) quando disponÃ­vel
2. **Faz fallback automÃ¡tico** para API externa (Gemini) se necessÃ¡rio
3. **MantÃ©m compatibilidade** total com o cÃ³digo existente

## ğŸ—ï¸ Arquitetura

### MÃ³dulos Criados

```
services/
â”œâ”€â”€ localAIService.ts      # ServiÃ§o de comunicaÃ§Ã£o com Ollama
â”œâ”€â”€ iaController.ts         # Controlador com fallback automÃ¡tico
â””â”€â”€ geminiService.ts        # Modificado para usar IAController

local-server/
â”œâ”€â”€ install_model.ps1       # Script de instalaÃ§Ã£o (Windows)
â”œâ”€â”€ install_model.sh        # Script de instalaÃ§Ã£o (Linux/macOS)
â”œâ”€â”€ start_local_ia.ps1     # Script de inicializaÃ§Ã£o (Windows)
â”œâ”€â”€ start_local_ia.sh      # Script de inicializaÃ§Ã£o (Linux/macOS)
â””â”€â”€ README.md               # DocumentaÃ§Ã£o dos scripts

pages/
â””â”€â”€ SettingsPage.tsx        # Adicionada seÃ§Ã£o de IA Local
```

### Fluxo de DecisÃ£o

```
UsuÃ¡rio solicita IA
    â†“
IAController verifica preferÃªncia
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ IA Local habilitada?        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ SIM                    â†“ NÃƒO
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Tentar Local â”‚      â”‚ Usar API Ext â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Sucesso?    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ SIM                    â†“ NÃƒO
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Retornar     â”‚      â”‚ Fallback API â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”§ IntegraÃ§Ã£o nos ServiÃ§os

### 1. `geminiService.ts`

**ModificaÃ§Ã£o mÃ­nima** na funÃ§Ã£o `generateMealPlan`:

```typescript
// ANTES: Chamava API diretamente
const response = await ai.models.generateContent({...});

// DEPOIS: Usa IAController com fallback
const localResponse = await generateJSONResponse<GeminiMealPlanResponse>(
    prompt,
    systemPrompt,
    async () => {
        // Fallback para API externa
        return await ai.models.generateContent({...});
    }
);
```

**O que foi mantido:**
- âœ… Toda a lÃ³gica existente
- âœ… Tratamento de erros
- âœ… Fallback offline
- âœ… Compatibilidade total

### 2. `assistantService.ts`

**NÃ£o modificado** - pode ser integrado no futuro se necessÃ¡rio.

### 3. `SettingsPage.tsx`

**Adicionada seÃ§Ã£o** de configuraÃ§Ã£o de IA Local:

- Checkbox para habilitar/desabilitar
- BotÃ£o de teste
- InstruÃ§Ãµes de instalaÃ§Ã£o
- Feedback visual do status

## ğŸš€ Como Usar

### InstalaÃ§Ã£o

1. **Windows:**
   ```powershell
   cd local-server
   .\install_model.ps1
   ```

2. **Linux/macOS:**
   ```bash
   cd local-server
   chmod +x install_model.sh
   ./install_model.sh
   ```

### ConfiguraÃ§Ã£o no App

1. Abra o app
2. VÃ¡ em **ConfiguraÃ§Ãµes**
3. Ative **"Usar IA Local Offline (Ollama)"**
4. Clique em **"Testar IA Local"** para verificar

### Uso AutomÃ¡tico

Uma vez configurado, o app **automaticamente**:

- Tenta usar IA Local primeiro (se habilitada)
- Faz fallback para API externa se necessÃ¡rio
- Usa modo offline se tudo falhar

## ğŸ“ Detalhes TÃ©cnicos

### `localAIService.ts`

**FunÃ§Ãµes principais:**
- `checkOllamaHealth()` - Verifica se Ollama estÃ¡ rodando
- `generateLocalResponse()` - Gera resposta usando Ollama
- `testLocalIA()` - Testa a conexÃ£o
- `getAvailableModel()` - ObtÃ©m modelo disponÃ­vel

**Modelos suportados:**
- `phi3:mini` (recomendado - ~2.3 GB)
- `llama3.1:8b` (~4.7 GB)
- `mistral:7b` (~4.1 GB)
- `qwen2.5:7b` (~4.4 GB)

### `iaController.ts`

**FunÃ§Ãµes principais:**
- `generateResponse()` - Gera resposta com fallback
- `generateJSONResponse()` - Gera JSON estruturado
- `shouldUseLocalAI()` - Verifica preferÃªncia do usuÃ¡rio
- `setUseLocalAI()` - Define preferÃªncia

**ConfiguraÃ§Ã£o:**
- Armazenada em `localStorage` com chave `nutria.useLocalAI`
- PadrÃ£o: `true` (IA Local Ã© primeira opÃ§Ã£o)

## ğŸ”„ Fallback AutomÃ¡tico

O sistema de fallback funciona em 3 nÃ­veis:

1. **IA Local (Ollama)**
   - Verifica se estÃ¡ habilitada
   - Verifica se Ollama estÃ¡ rodando
   - Tenta gerar resposta
   - Se falhar â†’ prÃ³ximo nÃ­vel

2. **API Externa (Gemini)**
   - Verifica se hÃ¡ API key
   - Chama API do Gemini
   - Se falhar â†’ prÃ³ximo nÃ­vel

3. **Modo Offline**
   - Usa respostas prÃ©-definidas
   - Sempre disponÃ­vel

## ğŸ›¡ï¸ Compatibilidade

### O que NÃƒO foi alterado:

- âœ… Estrutura de pastas
- âœ… Componentes existentes
- âœ… LÃ³gica de negÃ³cio
- âœ… Tratamento de erros
- âœ… Fallback offline existente

### O que foi adicionado:

- âœ… Novos serviÃ§os (`localAIService.ts`, `iaController.ts`)
- âœ… Scripts de instalaÃ§Ã£o
- âœ… SeÃ§Ã£o nas configuraÃ§Ãµes
- âœ… IntegraÃ§Ã£o opcional em `geminiService.ts`

## ğŸ§ª Testes

### Testar IA Local

1. Inicie o servidor Ollama:
   ```bash
   # Windows
   .\start_local_ia.ps1
   
   # Linux/macOS
   ./start_local_ia.sh
   ```

2. No app, vÃ¡ em **ConfiguraÃ§Ãµes**
3. Clique em **"Testar IA Local"**
4. Deve mostrar: "IA Local estÃ¡ funcionando! Modelo: phi3:mini"

### Testar Fallback

1. Desabilite IA Local nas configuraÃ§Ãµes
2. Use o app normalmente
3. Deve usar API externa como antes

## ğŸ“Š Performance

### IA Local (Ollama)

- **Vantagens:**
  - Funciona offline
  - Sem custos de API
  - Privacidade total
  - Sem limites de uso

- **Desvantagens:**
  - Requer instalaÃ§Ã£o
  - Consome recursos locais
  - Pode ser mais lento (depende do hardware)

### API Externa (Gemini)

- **Vantagens:**
  - Mais rÃ¡pida
  - NÃ£o consome recursos locais
  - Modelos mais poderosos

- **Desvantagens:**
  - Requer internet
  - Pode ter custos
  - Limites de uso

## ğŸ” SoluÃ§Ã£o de Problemas

### Ollama nÃ£o inicia

1. Verifique instalaÃ§Ã£o: `ollama --version`
2. Tente iniciar manualmente: `ollama serve`
3. Verifique porta 11434: `curl http://localhost:11434/api/tags`

### Modelo nÃ£o encontrado

1. Liste modelos: `ollama list`
2. Instale modelo: `ollama pull phi3:mini`

### App nÃ£o usa IA Local

1. Verifique se estÃ¡ habilitada nas ConfiguraÃ§Ãµes
2. Verifique se Ollama estÃ¡ rodando
3. Teste a conexÃ£o: clique em "Testar IA Local"

## ğŸ”— Links Ãšteis

- [DocumentaÃ§Ã£o do Ollama](https://github.com/ollama/ollama)
- [Lista de Modelos](https://ollama.com/library)
- [Guia de InstalaÃ§Ã£o](https://ollama.com/download)

## ğŸ“ Notas de ImplementaÃ§Ã£o

### DecisÃµes de Design

1. **NÃ£o invasivo:** A integraÃ§Ã£o foi feita de forma que o app continue funcionando normalmente mesmo sem Ollama
2. **Fallback inteligente:** Sistema de 3 nÃ­veis garante que sempre hÃ¡ uma opÃ§Ã£o disponÃ­vel
3. **ConfigurÃ¡vel:** UsuÃ¡rio pode escolher preferir IA Local ou API externa
4. **Modular:** ServiÃ§os separados facilitam manutenÃ§Ã£o e testes

### PrÃ³ximos Passos (Opcional)

- [ ] Integrar IA Local no chatbot (`assistantService.ts`)
- [ ] Adicionar streaming para IA Local
- [ ] Suporte a mÃºltiplos modelos
- [ ] Cache de respostas locais
- [ ] MÃ©tricas de performance

---

**Integrado em:** 2025-01-13  
**VersÃ£o:** 1.0  
**Status:** âœ… Funcional e Testado

