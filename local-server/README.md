# IA Local Offline - Documenta√ß√£o

Este diret√≥rio cont√©m scripts para instalar e gerenciar o servidor Ollama local, que permite usar IA offline sem depender de APIs externas.

## üìã Pr√©-requisitos

- Windows 10/11 ou Linux/macOS
- Conex√£o com internet (apenas para instala√ß√£o inicial)
- ~2-4 GB de espa√ßo em disco (para o modelo)

## üöÄ Instala√ß√£o R√°pida

### Windows

1. Abra o PowerShell como Administrador
2. Execute:
   ```powershell
   cd local-server
   .\install_model.ps1
   ```

### Linux/macOS

1. Abra o terminal
2. Execute:
   ```bash
   cd local-server
   chmod +x install_model.sh start_local_ia.sh
   ./install_model.sh
   ```

## üéØ Modelos Recomendados

O script instalar√° automaticamente um dos seguintes modelos (em ordem de prefer√™ncia):

1. **phi3:mini** (~2.3 GB) - Modelo leve e r√°pido, recomendado
2. **llama3.1:8b** (~4.7 GB) - Modelo mais poderoso
3. **mistral:7b** (~4.1 GB) - Boa alternativa
4. **qwen2.5:7b** (~4.4 GB) - Outra op√ß√£o

## ‚ñ∂Ô∏è Iniciar o Servidor

### Windows

```powershell
.\start_local_ia.ps1
```

### Linux/macOS

```bash
./start_local_ia.sh
```

O servidor estar√° dispon√≠vel em: `http://localhost:11434`

## üß™ Testar a Instala√ß√£o

Ap√≥s instalar e iniciar, teste com:

```bash
ollama run phi3:mini "Ol√°, como voc√™ est√°?"
```

Ou use a op√ß√£o de teste nas Configura√ß√µes do app.

## ‚öôÔ∏è Configura√ß√£o no App

1. Abra o app
2. V√° em **Configura√ß√µes**
3. Ative a op√ß√£o **"Usar IA Local Offline"**
4. Clique em **"Testar IA Local"** para verificar se est√° funcionando

## üîÑ Fallback Autom√°tico

O app usa um sistema de fallback inteligente:

1. **Primeira op√ß√£o**: IA Local (Ollama) - se estiver habilitada e dispon√≠vel
2. **Fallback**: API Externa (Gemini) - se IA Local falhar ou n√£o estiver dispon√≠vel
3. **√öltimo recurso**: Modo Offline - respostas pr√©-definidas

## üõ†Ô∏è Comandos √öteis

### Listar modelos instalados
```bash
ollama list
```

### Baixar um modelo espec√≠fico
```bash
ollama pull phi3:mini
```

### Remover um modelo
```bash
ollama rm phi3:mini
```

### Ver informa√ß√µes do modelo
```bash
ollama show phi3:mini
```

## üìù Notas

- O servidor Ollama precisa estar rodando para usar IA Local
- A primeira execu√ß√£o pode ser mais lenta (carregamento do modelo)
- Modelos maiores oferecem melhor qualidade, mas s√£o mais lentos
- Recomendamos usar `phi3:mini` para melhor equil√≠brio entre velocidade e qualidade

## ‚ùì Solu√ß√£o de Problemas

### Ollama n√£o inicia

1. Verifique se est√° instalado: `ollama --version`
2. Tente iniciar manualmente: `ollama serve`
3. Verifique se a porta 11434 est√° livre

### Modelo n√£o encontrado

1. Liste modelos: `ollama list`
2. Instale o modelo: `ollama pull phi3:mini`

### Erro de conex√£o no app

1. Verifique se o servidor est√° rodando: `curl http://localhost:11434/api/tags`
2. Verifique o firewall (porta 11434)
3. Tente reiniciar o servidor

## üîó Links √öteis

- [Documenta√ß√£o do Ollama](https://github.com/ollama/ollama)
- [Lista de Modelos](https://ollama.com/library)
- [Guia de Instala√ß√£o](https://ollama.com/download)

